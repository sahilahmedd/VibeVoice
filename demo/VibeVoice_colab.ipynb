{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sahilahmedd/VibeVoice/blob/main/demo/VibeVoice_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WvIaUJD2y0yU",
      "metadata": {
        "id": "WvIaUJD2y0yU"
      },
      "source": [
        "# VibeVoice Colab — T4 Quickstart (1.5B)\n",
        "\n",
        "This notebook provides a quickstart guide to run VibeVoice on Colab with T4. The T4 GPU can only support the 1.5B model due to memory limitations. Please note that T4 can only use SDPA instead of flash_attention_2, which may result in unstable and lower audio quality. For the best TTS experience, we recommend trying the 7B model on a more powerful GPU.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8fTKYGx7DZk",
      "metadata": {
        "id": "e8fTKYGx7DZk"
      },
      "source": [
        "## Step 1: Setup Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "4wxJ6QHM-ZOb",
      "metadata": {
        "id": "4wxJ6QHM-ZOb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21ec0e87-2aa6-4b3c-b9e9-f78474001f1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ T4 GPU detected\n",
            "✅ Cloned VibeVoice repository\n",
            "\u001b[1m\u001b[31merror\u001b[39m\u001b[0m: /content/VibeVoice does not appear to be a Python project, as neither `pyproject.toml` nor `setup.py` are present in the directory\n",
            "✅ Installed dependencies\n",
            "Cancellation requested; stopping current tasks.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/hf\", line 10, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/cli/hf.py\", line 59, in main\n",
            "    service.run()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/cli/download.py\", line 132, in run\n",
            "    print(self._download())  # Print path to downloaded files\n",
            "          ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/cli/download.py\", line 169, in _download\n",
            "    return snapshot_download(\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/_snapshot_download.py\", line 332, in snapshot_download\n",
            "    thread_map(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/tqdm/contrib/concurrent.py\", line 69, in thread_map\n",
            "    return _executor_map(ThreadPoolExecutor, fn, *iterables, **tqdm_kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/tqdm/contrib/concurrent.py\", line 51, in _executor_map\n",
            "    return list(tqdm_class(ex.map(fn, *iterables, chunksize=chunksize), **kwargs))\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/tqdm/std.py\", line 1169, in __iter__\n",
            "    for obj in iterable:\n",
            "               ^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/concurrent/futures/_base.py\", line 619, in result_iterator\n",
            "    yield _result_or_cancel(fs.pop())\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/concurrent/futures/_base.py\", line 317, in _result_or_cancel\n",
            "    return fut.result(timeout)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/concurrent/futures/_base.py\", line 451, in result\n",
            "    self._condition.wait(timeout)\n",
            "  File \"/usr/lib/python3.12/threading.py\", line 355, in wait\n",
            "    waiter.acquire()\n",
            "KeyboardInterrupt\n",
            "^C\n",
            "✅ Downloaded model: microsoft/VibeVoice-1.5B\n"
          ]
        }
      ],
      "source": [
        "# Check for T4 GPU\n",
        "import torch\n",
        "if torch.cuda.is_available() and \"T4\" in torch.cuda.get_device_name(0):\n",
        "    print(\"✅ T4 GPU detected\")\n",
        "else:\n",
        "    print(\"\"\"\n",
        "    ⚠️ WARNING: T4 GPU not detected\n",
        "\n",
        "    The recommended runtime for this Colab notebook is \"T4 GPU\".\n",
        "\n",
        "    To change the runtime type:\n",
        "\n",
        "        1. Click on \"Runtime\" in the top navigation menu\n",
        "        2. Click on \"Change runtime type\"\n",
        "        3. Select \"T4 GPU\"\n",
        "        4. Click \"OK\" if a \"Disconnect and delete runtime\" window appears\n",
        "        5. Click on \"Save\"\n",
        "\n",
        "    \"\"\")\n",
        "\n",
        "# Clone the VibeVoice repository\n",
        "![ -d /content/VibeVoice ] || git clone --quiet --branch main --depth 1 https://github.com/JarodMica/VibeVoice.git /content/VibeVoice\n",
        "print(\"✅ Cloned VibeVoice repository\")\n",
        "\n",
        "# Install project dependencies\n",
        "!uv pip --quiet install -e /content/VibeVoice --no-build-isolation\n",
        "print(\"✅ Installed dependencies\")\n",
        "\n",
        "# Download model (~3 minutes)\n",
        "!HF_XET_HIGH_PERFORMANCE=1 hf download microsoft/VibeVoice-1.5B --quiet  --local-dir /content/models/VibeVoice-1.5B > /dev/null\n",
        "print(\"✅ Downloaded model: microsoft/VibeVoice-1.5B\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pgKlV7153Ifi",
      "metadata": {
        "id": "pgKlV7153Ifi"
      },
      "source": [
        "## Step 2: Create Transcript"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "Yc1N9EHswFxA",
      "metadata": {
        "id": "Yc1N9EHswFxA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a31a78f-c85d-41d0-9922-dc35e8e41ff4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/my_transcript.txt\n"
          ]
        }
      ],
      "source": [
        "%%writefile /content/my_transcript.txt\n",
        "Speaker 1: Can I try VibeVoice with my own example?\n",
        "Speaker 2: Of course! VibeVoice is open-source, built to benefit everyone - you're welcome to try it out.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers==4.44.2 accelerate==0.34.2"
      ],
      "metadata": {
        "id": "F3MTbVJ_qwQN"
      },
      "id": "F3MTbVJ_qwQN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "MBCC6s-F6_hP",
      "metadata": {
        "id": "MBCC6s-F6_hP"
      },
      "source": [
        "## Step 3: Generate Audio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "dYWsLJ-n0Npm",
      "metadata": {
        "id": "dYWsLJ-n0Npm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "13f66805-a797-42d8-bd9a-ef66f9831910"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-02 13:01:53.900052: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1762088514.142582    2571 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1762088514.205149    2571 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1762088514.682671    2571 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762088514.682705    2571 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762088514.682711    2571 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762088514.682715    2571 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-02 13:01:54.687477: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "WARNING:vibevoice.modular.modular_vibevoice_tokenizer:APEX FusedRMSNorm not available, using native implementation\n",
            "Using device: cuda\n",
            "Found 9 voice files in /content/VibeVoice/demo/voices\n",
            "Available voices: en-Alice_woman, en-Carter_man, en-Frank_man, en-Mary_woman_bgm, en-Maya_woman, in-Samuel_man, zh-Anchen_man_bgm, zh-Bowen_man, zh-Xinran_woman\n",
            "Reading script from: /content/my_transcript.txt\n",
            "Found 2 speaker segments:\n",
            "  1. Speaker 1\n",
            "     Text preview: Speaker 1: Can I try VibeVoice with my own example?...\n",
            "  2. Speaker 2\n",
            "     Text preview: Speaker 2: Of course! VibeVoice is open-source, built to benefit everyone - you're welcome to try it...\n",
            "\n",
            "Speaker mapping:\n",
            "  Speaker 1 -> Alice\n",
            "  Speaker 2 -> Frank\n",
            "Speaker 1 ('Alice') -> Voice: en-Alice_woman.wav\n",
            "Speaker 2 ('Frank') -> Voice: en-Frank_man.wav\n",
            "Loading processor & model from /content/models/VibeVoice-1.5B\n",
            "tokenizer_config.json: 7.23kB [00:00, 26.9MB/s]\n",
            "vocab.json: 2.78MB [00:00, 78.7MB/s]\n",
            "merges.txt: 1.67MB [00:00, 138MB/s]\n",
            "tokenizer.json: 7.03MB [00:00, 188MB/s]\n",
            "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B/snapshots/8faed761d45a263340a0528343f099c05c9a4323/vocab.json\n",
            "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B/snapshots/8faed761d45a263340a0528343f099c05c9a4323/merges.txt\n",
            "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B/snapshots/8faed761d45a263340a0528343f099c05c9a4323/tokenizer.json\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B/snapshots/8faed761d45a263340a0528343f099c05c9a4323/tokenizer_config.json\n",
            "loading file chat_template.jinja from cache at None\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'Qwen2Tokenizer'. \n",
            "The class this function is called from is 'VibeVoiceTextTokenizerFast'.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Using device: cuda, torch_dtype: torch.bfloat16, attn_implementation: flash_attention_2\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "loading configuration file /content/models/VibeVoice-1.5B/config.json\n",
            "Model config VibeVoiceConfig {\n",
            "  \"_attn_implementation_autoset\": false,\n",
            "  \"acoustic_tokenizer_config\": {\n",
            "    \"causal\": true,\n",
            "    \"channels\": 1,\n",
            "    \"conv_bias\": true,\n",
            "    \"conv_norm\": \"none\",\n",
            "    \"corpus_normalize\": 0.0,\n",
            "    \"decoder_depths\": null,\n",
            "    \"decoder_n_filters\": 32,\n",
            "    \"decoder_ratios\": [\n",
            "      8,\n",
            "      5,\n",
            "      5,\n",
            "      4,\n",
            "      2,\n",
            "      2\n",
            "    ],\n",
            "    \"disable_last_norm\": true,\n",
            "    \"encoder_depths\": \"3-3-3-3-3-3-8\",\n",
            "    \"encoder_n_filters\": 32,\n",
            "    \"encoder_ratios\": [\n",
            "      8,\n",
            "      5,\n",
            "      5,\n",
            "      4,\n",
            "      2,\n",
            "      2\n",
            "    ],\n",
            "    \"fix_std\": 0.5,\n",
            "    \"layer_scale_init_value\": 1e-06,\n",
            "    \"layernorm\": \"RMSNorm\",\n",
            "    \"layernorm_elementwise_affine\": true,\n",
            "    \"layernorm_eps\": 1e-05,\n",
            "    \"mixer_layer\": \"depthwise_conv\",\n",
            "    \"model_type\": \"vibevoice_acoustic_tokenizer\",\n",
            "    \"pad_mode\": \"constant\",\n",
            "    \"std_dist_type\": \"gaussian\",\n",
            "    \"vae_dim\": 64,\n",
            "    \"weight_init_value\": 0.01\n",
            "  },\n",
            "  \"acoustic_vae_dim\": 64,\n",
            "  \"architectures\": [\n",
            "    \"VibeVoiceForConditionalGeneration\"\n",
            "  ],\n",
            "  \"decoder_config\": {\n",
            "    \"attention_dropout\": 0.0,\n",
            "    \"dtype\": \"bfloat16\",\n",
            "    \"hidden_act\": \"silu\",\n",
            "    \"hidden_size\": 1536,\n",
            "    \"initializer_range\": 0.02,\n",
            "    \"intermediate_size\": 8960,\n",
            "    \"layer_types\": [\n",
            "      \"full_attention\",\n",
            "      \"full_attention\",\n",
            "      \"full_attention\",\n",
            "      \"full_attention\",\n",
            "      \"full_attention\",\n",
            "      \"full_attention\",\n",
            "      \"full_attention\",\n",
            "      \"full_attention\",\n",
            "      \"full_attention\",\n",
            "      \"full_attention\",\n",
            "      \"full_attention\",\n",
            "      \"full_attention\",\n",
            "      \"full_attention\",\n",
            "      \"full_attention\",\n",
            "      \"full_attention\",\n",
            "      \"full_attention\",\n",
            "      \"full_attention\",\n",
            "      \"full_attention\",\n",
            "      \"full_attention\",\n",
            "      \"full_attention\",\n",
            "      \"full_attention\",\n",
            "      \"full_attention\",\n",
            "      \"full_attention\",\n",
            "      \"full_attention\",\n",
            "      \"full_attention\",\n",
            "      \"full_attention\",\n",
            "      \"full_attention\",\n",
            "      \"full_attention\"\n",
            "    ],\n",
            "    \"max_position_embeddings\": 65536,\n",
            "    \"max_window_layers\": 28,\n",
            "    \"model_type\": \"qwen2\",\n",
            "    \"num_attention_heads\": 12,\n",
            "    \"num_hidden_layers\": 28,\n",
            "    \"num_key_value_heads\": 2,\n",
            "    \"rms_norm_eps\": 1e-06,\n",
            "    \"rope_scaling\": null,\n",
            "    \"rope_theta\": 1000000.0,\n",
            "    \"sliding_window\": null,\n",
            "    \"tie_word_embeddings\": true,\n",
            "    \"use_cache\": true,\n",
            "    \"use_sliding_window\": false,\n",
            "    \"vocab_size\": 151936\n",
            "  },\n",
            "  \"diffusion_head_config\": {\n",
            "    \"ddpm_batch_mul\": 4,\n",
            "    \"ddpm_beta_schedule\": \"cosine\",\n",
            "    \"ddpm_num_inference_steps\": 20,\n",
            "    \"ddpm_num_steps\": 1000,\n",
            "    \"diffusion_type\": \"ddpm\",\n",
            "    \"head_ffn_ratio\": 3.0,\n",
            "    \"head_layers\": 4,\n",
            "    \"hidden_size\": 1536,\n",
            "    \"latent_size\": 64,\n",
            "    \"model_type\": \"vibevoice_diffusion_head\",\n",
            "    \"prediction_type\": \"v_prediction\",\n",
            "    \"rms_norm_eps\": 1e-05,\n",
            "    \"speech_vae_dim\": 64\n",
            "  },\n",
            "  \"dtype\": \"bfloat16\",\n",
            "  \"model_type\": \"vibevoice\",\n",
            "  \"semantic_tokenizer_config\": {\n",
            "    \"causal\": true,\n",
            "    \"channels\": 1,\n",
            "    \"conv_bias\": true,\n",
            "    \"conv_norm\": \"none\",\n",
            "    \"corpus_normalize\": 0.0,\n",
            "    \"disable_last_norm\": true,\n",
            "    \"encoder_depths\": \"3-3-3-3-3-3-8\",\n",
            "    \"encoder_n_filters\": 32,\n",
            "    \"encoder_ratios\": [\n",
            "      8,\n",
            "      5,\n",
            "      5,\n",
            "      4,\n",
            "      2,\n",
            "      2\n",
            "    ],\n",
            "    \"fix_std\": 0,\n",
            "    \"layer_scale_init_value\": 1e-06,\n",
            "    \"layernorm\": \"RMSNorm\",\n",
            "    \"layernorm_elementwise_affine\": true,\n",
            "    \"layernorm_eps\": 1e-05,\n",
            "    \"mixer_layer\": \"depthwise_conv\",\n",
            "    \"model_type\": \"vibevoice_semantic_tokenizer\",\n",
            "    \"pad_mode\": \"constant\",\n",
            "    \"std_dist_type\": \"none\",\n",
            "    \"vae_dim\": 128,\n",
            "    \"weight_init_value\": 0.01\n",
            "  },\n",
            "  \"semantic_vae_dim\": 128,\n",
            "  \"transformers_version\": \"4.57.1\"\n",
            "}\n",
            "\n",
            "loading weights file /content/models/VibeVoice-1.5B/model.safetensors.index.json\n",
            "Instantiating VibeVoiceForConditionalGenerationInference model under default dtype torch.bfloat16.\n",
            "[ERROR] : ImportError: FlashAttention2 has been toggled on, but it cannot be used due to the following error: the package flash_attn seems to be not installed. Please refer to the documentation of https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install Flash Attention 2.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/VibeVoice/demo/inference_from_file.py\", line 282, in main\n",
            "    model = VibeVoiceForConditionalGenerationInference.from_pretrained(\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\", line 277, in _wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\", line 4971, in from_pretrained\n",
            "    model = cls(config, *model_args, **model_kwargs)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/VibeVoice/vibevoice/modular/modeling_vibevoice_inference.py\", line 73, in __init__\n",
            "    super().__init__(config)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\", line 2076, in __init__\n",
            "    self.config._attn_implementation_internal = self._check_and_adjust_attn_implementation(\n",
            "                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\", line 2686, in _check_and_adjust_attn_implementation\n",
            "    applicable_attn_implementation = self.get_correct_attn_implementation(\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\", line 2714, in get_correct_attn_implementation\n",
            "    self._flash_attn_2_can_dispatch(is_init_check)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\", line 2422, in _flash_attn_2_can_dispatch\n",
            "    raise ImportError(f\"{preface} the package flash_attn seems to be not installed. {install_message}\")\n",
            "ImportError: FlashAttention2 has been toggled on, but it cannot be used due to the following error: the package flash_attn seems to be not installed. Please refer to the documentation of https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install Flash Attention 2.\n",
            "\n",
            "Error loading the model. Trying to use SDPA. However, note that only flash_attention_2 has been fully tested, and using SDPA may result in lower audio quality.\n",
            "loading configuration file /content/models/VibeVoice-1.5B/config.json\n",
            "Model config VibeVoiceConfig {\n",
            "  \"_attn_implementation_autoset\": false,\n",
            "  \"acoustic_tokenizer_config\": {\n",
            "    \"causal\": true,\n",
            "    \"channels\": 1,\n",
            "    \"conv_bias\": true,\n",
            "    \"conv_norm\": \"none\",\n",
            "    \"corpus_normalize\": 0.0,\n",
            "    \"decoder_depths\": null,\n",
            "    \"decoder_n_filters\": 32,\n",
            "    \"decoder_ratios\": [\n",
            "      8,\n",
            "      5,\n",
            "      5,\n",
            "      4,\n",
            "      2,\n",
            "      2\n",
            "    ],\n",
            "    \"disable_last_norm\": true,\n",
            "    \"encoder_depths\": \"3-3-3-3-3-3-8\",\n",
            "    \"encoder_n_filters\": 32,\n",
            "    \"encoder_ratios\": [\n",
            "      8,\n",
            "      5,\n",
            "      5,\n",
            "      4,\n",
            "      2,\n",
            "      2\n",
            "    ],\n",
            "    \"fix_std\": 0.5,\n",
            "    \"layer_scale_init_value\": 1e-06,\n",
            "    \"layernorm\": \"RMSNorm\",\n",
            "    \"layernorm_elementwise_affine\": true,\n",
            "    \"layernorm_eps\": 1e-05,\n",
            "    \"mixer_layer\": \"depthwise_conv\",\n",
            "    \"model_type\": \"vibevoice_acoustic_tokenizer\",\n",
            "    \"pad_mode\": \"constant\",\n",
            "    \"std_dist_type\": \"gaussian\",\n",
            "    \"vae_dim\": 64,\n",
            "    \"weight_init_value\": 0.01\n",
            "  },\n",
            "  \"acoustic_vae_dim\": 64,\n",
            "  \"architectures\": [\n",
            "    \"VibeVoiceForConditionalGeneration\"\n",
            "  ],\n",
            "  \"decoder_config\": {\n",
            "    \"attention_dropout\": 0.0,\n",
            "    \"dtype\": \"bfloat16\",\n",
            "    \"hidden_act\": \"silu\",\n",
            "    \"hidden_size\": 1536,\n",
            "    \"initializer_range\": 0.02,\n",
            "    \"intermediate_size\": 8960,\n",
            "    \"layer_types\": [\n",
            "      \"full_attention\",\n",
            "      \"full_attention\",\n",
            "      \"full_attention\",\n",
            "      \"full_attention\",\n",
            "      \"full_attention\",\n",
            "      \"full_attention\",\n",
            "      \"full_attention\",\n",
            "      \"full_attention\",\n",
            "      \"full_attention\",\n",
            "      \"full_attention\",\n",
            "      \"full_attention\",\n",
            "      \"full_attention\",\n",
            "      \"full_attention\",\n",
            "      \"full_attention\",\n",
            "      \"full_attention\",\n",
            "      \"full_attention\",\n",
            "      \"full_attention\",\n",
            "      \"full_attention\",\n",
            "      \"full_attention\",\n",
            "      \"full_attention\",\n",
            "      \"full_attention\",\n",
            "      \"full_attention\",\n",
            "      \"full_attention\",\n",
            "      \"full_attention\",\n",
            "      \"full_attention\",\n",
            "      \"full_attention\",\n",
            "      \"full_attention\",\n",
            "      \"full_attention\"\n",
            "    ],\n",
            "    \"max_position_embeddings\": 65536,\n",
            "    \"max_window_layers\": 28,\n",
            "    \"model_type\": \"qwen2\",\n",
            "    \"num_attention_heads\": 12,\n",
            "    \"num_hidden_layers\": 28,\n",
            "    \"num_key_value_heads\": 2,\n",
            "    \"rms_norm_eps\": 1e-06,\n",
            "    \"rope_scaling\": null,\n",
            "    \"rope_theta\": 1000000.0,\n",
            "    \"sliding_window\": null,\n",
            "    \"tie_word_embeddings\": true,\n",
            "    \"use_cache\": true,\n",
            "    \"use_sliding_window\": false,\n",
            "    \"vocab_size\": 151936\n",
            "  },\n",
            "  \"diffusion_head_config\": {\n",
            "    \"ddpm_batch_mul\": 4,\n",
            "    \"ddpm_beta_schedule\": \"cosine\",\n",
            "    \"ddpm_num_inference_steps\": 20,\n",
            "    \"ddpm_num_steps\": 1000,\n",
            "    \"diffusion_type\": \"ddpm\",\n",
            "    \"head_ffn_ratio\": 3.0,\n",
            "    \"head_layers\": 4,\n",
            "    \"hidden_size\": 1536,\n",
            "    \"latent_size\": 64,\n",
            "    \"model_type\": \"vibevoice_diffusion_head\",\n",
            "    \"prediction_type\": \"v_prediction\",\n",
            "    \"rms_norm_eps\": 1e-05,\n",
            "    \"speech_vae_dim\": 64\n",
            "  },\n",
            "  \"dtype\": \"bfloat16\",\n",
            "  \"model_type\": \"vibevoice\",\n",
            "  \"semantic_tokenizer_config\": {\n",
            "    \"causal\": true,\n",
            "    \"channels\": 1,\n",
            "    \"conv_bias\": true,\n",
            "    \"conv_norm\": \"none\",\n",
            "    \"corpus_normalize\": 0.0,\n",
            "    \"disable_last_norm\": true,\n",
            "    \"encoder_depths\": \"3-3-3-3-3-3-8\",\n",
            "    \"encoder_n_filters\": 32,\n",
            "    \"encoder_ratios\": [\n",
            "      8,\n",
            "      5,\n",
            "      5,\n",
            "      4,\n",
            "      2,\n",
            "      2\n",
            "    ],\n",
            "    \"fix_std\": 0,\n",
            "    \"layer_scale_init_value\": 1e-06,\n",
            "    \"layernorm\": \"RMSNorm\",\n",
            "    \"layernorm_elementwise_affine\": true,\n",
            "    \"layernorm_eps\": 1e-05,\n",
            "    \"mixer_layer\": \"depthwise_conv\",\n",
            "    \"model_type\": \"vibevoice_semantic_tokenizer\",\n",
            "    \"pad_mode\": \"constant\",\n",
            "    \"std_dist_type\": \"none\",\n",
            "    \"vae_dim\": 128,\n",
            "    \"weight_init_value\": 0.01\n",
            "  },\n",
            "  \"semantic_vae_dim\": 128,\n",
            "  \"transformers_version\": \"4.57.1\"\n",
            "}\n",
            "\n",
            "loading weights file /content/models/VibeVoice-1.5B/model.safetensors.index.json\n",
            "Instantiating VibeVoiceForConditionalGenerationInference model under default dtype torch.bfloat16.\n",
            "Generate config GenerationConfig {}\n",
            "\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Instantiating Qwen2Model model under default dtype torch.bfloat16.\n",
            "Instantiating VibeVoiceAcousticTokenizerModel model under default dtype torch.bfloat16.\n",
            "Instantiating VibeVoiceSemanticTokenizerModel model under default dtype torch.bfloat16.\n",
            "Instantiating VibeVoiceDiffusionHead model under default dtype torch.bfloat16.\n",
            "Loading checkpoint shards: 100% 3/3 [00:19<00:00,  6.60s/it]\n",
            "Generation config file not found, using a generation config created from the model config.\n",
            "Could not locate the custom_generate/generate.py inside /content/models/VibeVoice-1.5B.\n",
            "Language model attention: sdpa\n",
            "Starting generation with cfg_scale: 1.3\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/VibeVoice/demo/inference_from_file.py\", line 400, in <module>\n",
            "    main()\n",
            "  File \"/content/VibeVoice/demo/inference_from_file.py\", line 337, in main\n",
            "    outputs = model.generate(\n",
            "              ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/VibeVoice/vibevoice/modular/modeling_vibevoice_inference.py\", line 373, in generate\n",
            "    generation_config, model_kwargs, input_ids, logits_processor, stopping_criteria = self._build_generate_config_model_kwargs(\n",
            "                                                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/VibeVoice/vibevoice/modular/modeling_vibevoice_inference.py\", line 303, in _build_generate_config_model_kwargs\n",
            "    self._prepare_cache_for_generation(generation_config, model_kwargs, None, batch_size, max_cache_length, device)\n",
            "TypeError: GenerationMixin._prepare_cache_for_generation() takes 6 positional arguments but 7 were given\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "rate must be specified when data is a numpy array or list of audio samples.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3274554393.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Display audio controls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAudio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mAudio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/outputs/my_transcript_generated.wav\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/lib/display.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, filename, url, embed, rate, autoplay, normalize, element_id)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mrate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rate must be specified when data is a numpy array or list of audio samples.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_wav\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: rate must be specified when data is a numpy array or list of audio samples."
          ]
        }
      ],
      "source": [
        "# Run Python script to generate audio from transcript\n",
        "!PYTHONPATH=/content/VibeVoice python /content/VibeVoice/demo/inference_from_file.py \\\n",
        "    --model_path /content/models/VibeVoice-1.5B \\\n",
        "    --txt_path /content/my_transcript.txt \\\n",
        "    --speaker_names Alice Frank\n",
        "\n",
        "# Display audio controls\n",
        "from IPython.display import Audio\n",
        "Audio(\"/content/outputs/my_transcript_generated.wav\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec6438d5",
      "metadata": {
        "id": "ec6438d5"
      },
      "source": [
        "# Step 4: Download Audio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b40ffa22",
      "metadata": {
        "id": "b40ffa22"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.download(\"/content/outputs/my_transcript_generated.wav\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1bce752d",
      "metadata": {
        "id": "1bce752d"
      },
      "source": [
        "\n",
        "## Risks and Limitations\n",
        "\n",
        "While efforts have been made to optimize it through various techniques, it may still produce outputs that are unexpected, biased, or inaccurate. VibeVoice inherits any biases, errors, or omissions produced by its base model (specifically, Qwen2.5 1.5b in this release). Potential for Deepfakes and Disinformation: High-quality synthetic speech can be misused to create convincing fake audio content for impersonation, fraud, or spreading disinformation. Users must ensure transcripts are reliable, check content accuracy, and avoid using generated content in misleading ways. Users are expected to use the generated content and to deploy the models in a lawful manner, in full compliance with all applicable laws and regulations in the relevant jurisdictions. It is best practice to disclose the use of AI when sharing AI-generated content."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "name": "VibeVoice_Colab.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}